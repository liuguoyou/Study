{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XNOR-Net\n",
    "[XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/pdf/1603.05279.pdf)\n",
    "\n",
    "## Related Work\n",
    "\n",
    "### Shallow networks\n",
    "\n",
    "### Compressing pre-trained deep networks\n",
    "\n",
    "主要思想是减少冗余的参数，有些神经元的参数比较小，作用也比较小，所以可以去掉。\n",
    "\n",
    "- Weight decay [cite 17]\n",
    "- the Hessian of loss function: Optimal Brain Damage [cite 18] and Optimal Brain Surgeon [cite 19]\n",
    "- ...\n",
    "\n",
    "### Designing compact layers\n",
    "\n",
    "研究网络结构，减少计算量。\n",
    "\n",
    "Using in Network in Network architecture, GoogLenet and Residual-Net\n",
    "\n",
    "In Residual-Net, bottleneck structure has benn proposed.\n",
    "\n",
    "### Quantizing parameters\n",
    "\n",
    "量化参数\n",
    "\n",
    "### Network binarization\n",
    "\n",
    "- Expectation BackPropagation(EBP) [cite 36]\n",
    "- Bayesian approach\n",
    "- BinaryConnect [cite 38]\n",
    "- BinaryNet [cite 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Convolutional Neural Network\n",
    "\n",
    "$L$-layer CNN architecture: $ <\\mathcal{I}, \\mathcal{W}, * > $\n",
    "\n",
    "$I = \\mathcal{I} _ {l(l=1,\\dots,L)}$ is the input tensor for the $l^{\\mathrm{th}}$ layer of CNN.\n",
    "\n",
    "$W = \\mathcal{W} _ {k=1,\\dots, K^l}$ is the $k^{\\mathrm{th}}$ weight filter in the $l^{\\mathrm{th}}$ layer of the CNN.\n",
    "\n",
    "$K^l$ is the number of weight filters in the $l^{\\mathrm{th}}$ layer of CNN.\n",
    "\n",
    "$*$ represents a convolutional operation with $I$ and $W$ as it operands(操作数).\n",
    "\n",
    "$I \\in \\mathbb{R}^{c \\times w_{in} \\times h_{in}}$, where $(c, w_{in}, h_{in})$ represents *channels, width* and *height* respectively.\n",
    "\n",
    "$W \\in \\mathbb{R} ^ {c \\times w \\times h}$, where $w \\leq w_{in}, h \\leq h_{in}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary-Weight-Networks\n",
    "\n",
    "**Result:**\n",
    "- is smaller ($\\sim 32 \\times$) than an equivalent network with single-precision weights values.\n",
    "- in $\\sim 2\\times$ speed up.\n",
    "\n",
    "**Goal:**\n",
    "$$W \\approx \\alpha B,\\;\\;\\;\\;\\alpha \\in \\mathbb{R}^{+}, B \\in \\{+1, -1\\}^{c \\times w \\times h}$$\n",
    "\n",
    "So, $$I * W \\approx (I \\oplus B) \\alpha$$\n",
    "where $\\oplus$ indicates a convolution without any multiplication.\n",
    "\n",
    "** optimization **\n",
    "$$B^{*} = \\mathrm{sign}(W)$$\n",
    "$$\\alpha^{*} = \\frac{1}{n} \\| W \\| _{\\ell 1} = \\frac{\\sum{|W|}}{n} = \\frac{\\sum{B \\odot W}}{n}$$\n",
    "\n",
    "** Train **\n",
    "\n",
    "$$\\frac{\\partial \\mathrm{sign}}{\\partial r} = r1_{|r| \\leq 1}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W} = \\frac{\\partial C}{\\tilde{W}}(\\frac{1}{n} + \\frac{\\partial \\mathrm{sign}}{\\partial W} \\alpha)$$\n",
    "\n",
    "let $$C = I*W \\approx I * (B \\alpha) = (I \\oplus B) \\alpha = M \\alpha, M = I \\oplus B$$\n",
    "so, $$\\frac{\\partial C}{\\partial \\alpha} = \\sum{(M \\odot C')}$$\n",
    "$$\\frac{\\partial \\alpha}{\\partial W} = B \\odot \\frac{1}{n}$$\n",
    "$$\\frac{\\partial C}{\\partial M} = \\alpha C'$$\n",
    "$$\\frac{\\partial M}{\\partial I} = (M * B)^{\\mathrm{flip}} \\odot M'$$\n",
    "$$\\frac{\\partial M}{\\partial B} = I * M'$$\n",
    "$$\\frac{\\partial B}{\\partial W} = \\frac{\\partial \\mathrm{sign}}{\\partial W}$$\n",
    "$$\\frac{\\partial C}{\\partial W} = \\frac{\\partial C}{\\partial \\alpha} \\frac{\\partial \\alpha}{\\partial W} + \\frac{\\partial C}{\\partial M}\\frac{\\partial M}{\\partial B} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XNOR-Networks\n",
    "\n",
    "**Result:**\n",
    "- $\\sim 58 \\times$ speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进思路\n",
    "\n",
    "1. 输入层改进：\n",
    "    - （原）实数输入，Batch Normalization之后直接输入\n",
    "    - 增广通道数，原图片每一像素的每一bit都作为一个通道，即3通道变为24通道\n",
    "    - 增加一实数卷积层，已增加通道数，保留更多的信息。\n",
    "    \n",
    "2. 卷积层改进\n",
    "    - 参数的二值化变为三值化，即{-1,0,1}，用多一倍的计算时间和空间换取精度。\n",
    "    - 尝试其他梯度反向传播函数\n",
    "    - padding对二值化的影响？\n",
    "\n",
    "3. 梯度二值化\n",
    "    - 改变反向传播方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
